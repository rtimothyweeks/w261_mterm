{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS Machine Learning at Scale\n",
    "## MidTerm Exam  \n",
    "\n",
    "4:00PM - 6:00PM(CT)\n",
    "October 19, 2016   \n",
    "Midterm\n",
    "\n",
    "MIDS Machine Learning at Scale\n",
    "\n",
    "\n",
    "\n",
    "### Please insert your contact information here\n",
    "__Insert you name here__           :   Reo Timothy Weeks\n",
    "\n",
    "__Insert you email here__          : timothy@ischool.berkeley.edu \n",
    "\n",
    "__Insert your  UC Berkeley ID here__: 26967981"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Instructions\n",
    "\n",
    "1. : Please insert Name and Email address in the first cell of this notebook\n",
    "2. : Please acknowledge receipt of exam by sending a quick email reply to the instructor\n",
    "3. : Review the submission form first to scope it out (it will take a 5-10 minutes to input your \n",
    "   answers and other information into this form): \n",
    "\n",
    "    + [Exam Submission form](http://goo.gl/forms/ggNYfRXz0t) \n",
    "\n",
    "4. : Please keep all your work and responses in ONE (1) notebook only (and submit via the submission form)\n",
    "5. : Please make sure that the NBViewer link for your Submission notebook works \n",
    "6. : Please submit your solutions and notebook via the following form:\n",
    "\n",
    "     + [Exam Submission form](http://goo.gl/forms/ggNYfRXz0t)\n",
    "\n",
    "7. : For the midterm you will need access to MrJob and Jupyter on your local machines or on AltaScale/AWS to complete some of the questions (like fill in the code to do X).\n",
    "8. : As for question types:\n",
    "    + Knowledge test Programmatic/doodle (take photos; embed the photos in your notebook) \n",
    "    + All programmatic questions can be run locally on your laptop (using MrJob only) or on the cluster\n",
    "\n",
    "9. : This is an open book exam meaning you can consult webpages and textbooks, class notes, slides etc. but you can not discuss with each other or any other person/group. If any collusion, then this will result in a zero grade and will be grounds for dismissal from the entire program. Please complete this exam by yourself within the time limit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam questions begins here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===Map-Reduce==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT1. Which of the following statememts about map-reduce are true? \n",
    "\n",
    "(I) If you only have 1 computer with 1 computing core, then map-reduce is unlikely to help   \n",
    "(II) If we run map-reduce using N single-core computers, then it is likely to get at least an N-Fold speedup compared to using 1 computer   \n",
    "(III) Because of network latency and other overhead associated with map-reduce, if we run map-reduce using N    computers, then we will get less than N-Fold speedup compared to using 1 computer   \n",
    "(IV) When using map-reduce for learning a naive Bayes classifier for SPAM classification, we usually use a single machine that accumulates the partial class and word stats from each of the map machines, in order to compute the final model.\n",
    "\n",
    "Please select one from the following that is most correct:\n",
    "\n",
    "* (a) I, II, III, IV\n",
    "* (b) I, III, IV\n",
    "* (c) I, III\n",
    "* (d) I,II, III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===Order inversion===\n",
    "\n",
    "### MT2. normalized product co-occurrence \n",
    "\n",
    "uppose you wish to write a MapReduce job that creates  normalized product co-occurrence (i.e., pairs of products that have been purchased together) data form a large transaction file of shopping baskets. In addition, we want the relative frequency of coocurring products. Given this scenario, to ensure that all (potentially many) reducers\n",
    "receive appropriate normalization factors (denominators)for a product\n",
    "in the most effcient order in their input streams (so as to minimize memory overhead on the reducer side), \n",
    "the mapper should emit/yield records according to which pattern for the product occurence totals:   \n",
    "\n",
    "(a) emit (\\*,product) count   \n",
    "(b) There is no need to use  order inversion here   \n",
    "(c) emit (product,\\*) count   \n",
    "(d) None of the above   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===Map-Reduce===\n",
    "\n",
    "### MT3. What is the input to the Reduce function in MRJob? Select the most correct choice.  \n",
    "\n",
    "   \n",
    "(a) An arbitrarily sized list of key/value pairs.    \n",
    "(b) One key and a list of some values associated with that key  \n",
    "(c) One key and a list of all values associated with that key.   \n",
    "(d) None of the above  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===Bayesian document classification===   \n",
    "  \n",
    "### MT4. When building a Bayesian document classifier, Laplace smoothing serves what purpose?   \n",
    "\n",
    "(a) It allows you to use your training data as your validation data.   \n",
    "(b) It prevents zero-products in the posterior distribution.  \n",
    "(c) It accounts for words that were missed by regular expressions.    \n",
    "(d) None of the above   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT5. Big Data\n",
    "Big data is defined as the voluminous amount of structured, unstructured or semi-structured data that has huge potential for mining but is so large that it cannot be processed nor stored using traditional (single computer) computing and storage systems. Big data is characterized by its high velocity, volume and variety that requires cost effective and innovative methods for information processing to draw meaningful business insights. More than the volume of the data – it is the nature of the data that defines whether it is considered as Big Data or not. What do the four V’s of Big Data denote? Here is a potential simple explanation for each of the four critical features of big data (some or all of which is correct):\n",
    "\n",
    "__Statements__ \n",
    "* (I)  Volume –Scale of data\n",
    "* (II) Velocity – Batch processing of data offline\n",
    "* (III)Variety – Different forms of data\n",
    "* (IV) Veracity –Uncertainty of data\n",
    "\n",
    "Which combination of the above statements is correct. Select a single correct response from the following :\n",
    "\n",
    "* (a) I, II, III, IV\n",
    "* (b) I, III, IV\n",
    "* (c) I, III\n",
    "* (d) I,II, III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT6. Combiners can be integral to the successful utilization of the Hadoop shuffle.  \n",
    "Using combiners result in what?  \n",
    "\n",
    "* (I) minimization of reducer workload     \n",
    "* (II) minimization of disk storage for mapper results   \n",
    "* (III) minimization of network traffic    \n",
    "* (IV) none of the above \n",
    "\n",
    "Select most correct option (i.e., select one option only) from the following:\n",
    "\n",
    "* (a) I   \n",
    "* (b) I, II and III   \n",
    "* (c) II and III   \n",
    "* (d) IV   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise similarity using K-L divergence\n",
    "\n",
    "In probability theory and information theory, the Kullback–Leibler divergence \n",
    "(also information divergence, information gain, relative entropy, KLIC, or KL divergence) \n",
    "is a non-symmetric measure of the difference between two probability distributions P and Q. \n",
    "Specifically, the Kullback–Leibler divergence of Q from P, denoted DKL(P\\‖Q), \n",
    "is a measure of the information lost when Q is used to approximate P:\n",
    "\n",
    "For discrete probability distributions P and Q, \n",
    "the Kullback–Leibler divergence of Q from P is defined to be\n",
    "\n",
    "    + KLDistance(P, Q) = Sum_over_item_i (P(i) log (P(i) / Q(i))      \n",
    "\n",
    "In the extreme cases, the KL Divergence is 1 when P and Q are maximally different\n",
    "and is 0 when the two distributions are exactly the same (follow the same distribution).\n",
    "\n",
    "For more information on K-L Divergence see:\n",
    "\n",
    "    + [K-L Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "\n",
    "For the next three question we will use an MRjob class for calculating pairwise similarity \n",
    "using K-L Divergence as the similarity measure:\n",
    "\n",
    "* Job 1: create inverted index (assume just two objects)\n",
    "* Job 2: calculate/accumulate the similarity of each pair of objects using K-L Divergence\n",
    "\n",
    "\n",
    "Using the following cells  then fill in the code for the first reducer to calculate \n",
    "the K-L divergence of objects (letter documents) in line1 and line2, i.e., KLD(Line1||line2).\n",
    "\n",
    "Here we ignore characters which are not alphabetical. And all alphabetical characters are lower-cased in the first mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kltext.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile kltext.txt\n",
    "1.Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from large volumes of data in various forms (data in various forms, data in various forms, data in various forms), either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, data mining and predictive analytics, as well as Knowledge Discovery in Databases.\n",
    "2.Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[2] Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions,[3]:2 rather than following strictly static program instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986122886681098"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.log(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!cat kltext.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kldivergence.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kldivergence.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "import numpy as np\n",
    "class kldivergence(MRJob):\n",
    "    \n",
    "    # process each string character by character\n",
    "    # the relative frequency of each character emitting Pr(character|str)\n",
    "    # for input record 1.abcbe\n",
    "    # emit \"a\"    [1, 0.2]\n",
    "    # emit \"b\"    [1, 0.4] etc...\n",
    "    def mapper1(self, _, line):\n",
    "        index = int(line.split('.',1)[0])\n",
    "        letter_list = re.sub(r\"[^A-Za-z]+\", '', line).lower()\n",
    "        count = {}\n",
    "        for l in letter_list:  \n",
    "            if l in count:\n",
    "                count[l] += 1\n",
    "            else:\n",
    "                count[l] = 1\n",
    "        for key in count:\n",
    "            yield key, [index, count[key]*1.0/len(letter_list)]\n",
    "\n",
    "    # on a component i calculate (e.g., \"b\")\n",
    "    # Kullback–Leibler divergence of Q from P is defined\n",
    "    #  (P(i) log (P(i) / Q(i))\n",
    "    #\n",
    "    def reducer1(self, key, values):\n",
    "        watch_letters = ['p','t','k','q','j','f']\n",
    "        if key in watch_letters:\n",
    "            print(key)\n",
    "            watch_letters.remove(key)\n",
    "        p = 0\n",
    "        q = 0\n",
    "        for v in values:\n",
    "            if v[0] == 1:  #String 1\n",
    "                p = v[1]\n",
    "            else:          # String 2\n",
    "                q = v[1]\n",
    "                \n",
    "        yield v[0], p * np.log(p/q)\n",
    "\n",
    "    #Aggegate components            \n",
    "    def reducer2(self, key, values):\n",
    "        kl_sum = 0\n",
    "        for value in values:\n",
    "            kl_sum = kl_sum + value\n",
    "        yield \"KLDivergence\", kl_sum\n",
    "            \n",
    "    def steps(self):\n",
    "        return [self.mr(mapper=self.mapper1,\n",
    "                        reducer=self.reducer1),\n",
    "                \n",
    "                self.mr(reducer=self.reducer2)\n",
    "               \n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kldivergence.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n",
      "k\n",
      "p\n",
      "t\n",
      "('KLDivergence', 0.0808827844)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from mrjob.job import MRJob\n",
    "from kldivergence import kldivergence\n",
    "\n",
    "#dont forget to save kltext.txt (see earlier cell)\n",
    "mr_job = kldivergence(args=['kltext.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print(mr_job.parse_output_line(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT7. Which number below is the closest to the result you get for KLD(Line1||line2)?   \n",
    "(a) 0.7   \n",
    "(b) 0.5   \n",
    "(c) 0.2   \n",
    "(d) 0.1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT8. Which of the following letters are missing from these character vectors?   \n",
    "(a) p and t   \n",
    "(b) k and q   \n",
    "(c) j and q   \n",
    "(d) j and f   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kldivergence_smooth.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kldivergence_smooth.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "import numpy as np\n",
    "class kldivergence_smooth(MRJob):\n",
    "    \n",
    "    # process each string character by character\n",
    "    # the relative frequency of each character emitting Pr(character|str)\n",
    "    # for input record 1.abcbe\n",
    "    # emit \"a\"    [1, (1+1)/(5+24)]\n",
    "    # emit \"b\"    [1, (2+1)/(5+24) etc...\n",
    "    def mapper1(self, _, line):\n",
    "        index = int(line.split('.',1)[0])\n",
    "        letter_list = re.sub(r\"[^A-Za-z]+\", '', line).lower()\n",
    "        count = {}\n",
    "        \n",
    "        # (ni+1)/(n+24)\n",
    "        \n",
    "        for l in letter_list:\n",
    "            if l in count:\n",
    "                count[l] += 1\n",
    "            else:\n",
    "                count[l] = 1\n",
    "        for key in count:\n",
    "            yield key, [index, (count[key]*1.0 + 1) / (len(letter_list) + 24)]\n",
    "#             yield key, [index, count[key]*1.0/len(letter_list)]\n",
    "    \n",
    "    def reducer1(self, key, values):\n",
    "        p = 0\n",
    "        q = 0\n",
    "        for v in values:\n",
    "            if v[0] == 1:\n",
    "                p = v[1]\n",
    "            else:\n",
    "                q = v[1]\n",
    "                \n",
    "        yield v[0], p * np.log(p/q)\n",
    "\n",
    "    # Aggregate components             \n",
    "    def reducer2(self, key, values):\n",
    "        kl_sum = 0\n",
    "        for value in values:\n",
    "            kl_sum = kl_sum + value\n",
    "        yield \"KLDivergence\", kl_sum\n",
    "            \n",
    "    def steps(self):\n",
    "        return [self.mr(mapper=self.mapper1,\n",
    "                        reducer=self.reducer1),\n",
    "                self.mr(reducer=self.reducer2)\n",
    "               \n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kldivergence_smooth.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('KLDivergence', 0.0672699729)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from kldivergence_smooth import kldivergence_smooth\n",
    "mr_job = kldivergence_smooth(args=['kltext.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print(mr_job.parse_output_line(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT9. The KL divergence on multinomials is defined only when they have nonzero entries. \n",
    "For zero entries, we have to smooth distributions. Suppose we smooth in this way:    \n",
    "\n",
    "(ni+1)/(n+24)   \n",
    "\n",
    "where ni is the count for letter i and n is the total count of all letters.    \n",
    "After smoothing, which number below is the closest to the result you get for KLD(Line1||line2)??   \n",
    "\n",
    "(a) 0.08   \n",
    "(b) 0.71     \n",
    "(c) 0.02    \n",
    "(d) 0.11   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT10. Block size, and mapper tasks\n",
    "Given ten (10) files in the input directory for a Hadoop Streaming job (MRjob or just Hadoop) with the following filesizes (in megabytes): 1, 2,3,4,5,6,7,8,9,10; and a block size of 5M (NOTE: normally we should set the blocksize to 1 GigB using modern computers). How many map tasks will result from processing the data in the input directory? Select the closest number from the following list.\n",
    "\n",
    " (a) 1 map task  \n",
    " (b) 14  \n",
    " (c) 12   \n",
    " (d) None of the above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT11. Aggregation\n",
    "Given a purchase transaction log file where each purchase transaction contains the customer identifier, item purchased and much more information about the transaction. Which of the following statements are true about a MapReduce job that performs an  “aggregation” such as get the number of transaction per customer.\n",
    "\n",
    "__Statements__\n",
    "* (I) A mapper only job will not suffice, as each map tast only gets to see a subset of the data (e.g., one block). As such a mapper only job will only produce intermediate tallys for each customer. \n",
    "* (II) A reducer only job will suffice and is most efficient computationally\n",
    "* (III) If the developer provides a  Mapper and Reducer it can potentially be more efficient than option II\n",
    "* (IV) A reducer only job with a custom partitioner will suffice.\n",
    "\n",
    "Select the most correct option from the following:\n",
    "\n",
    "* (a) I, II, III, IV\n",
    "* (b) II, IV\n",
    "* (c) III, IV\n",
    "* (d) III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT12. Naive Bayes\n",
    "Which of the following statements are true regarding Naive Bayes?\n",
    "\n",
    "__Statements__\n",
    "* (I) Naive Bayes is a machine learning algorithm that can be used for classifcation problems only\n",
    "* (II) Multinomial Naive Bayes is a flavour of Naive Bayes for discrete input variables and can be combined  with Laplace smoothing to avoid zero predictions for class posterior probabilities  when attribute value combinations show up during classification but were not present during training. \n",
    "* (III) Naive Bayes can be used for continous valued input variables. In this case, one can use Gaussian distributions to model the class conditional probability distributions Pr(X|Class).\n",
    "* (IV) Naive Bayes can model continous target variables directly.\n",
    "\n",
    "Please select the single most correct combination from the following:\n",
    "    \n",
    "\n",
    "* (a) I, II, III, IV\n",
    "* (b) I, II, III\n",
    "* (c) I, III, IV\n",
    "* (d) I, II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT13. Naive Bayes SPAM model\n",
    "Given the following document dataset for a  Two-Class problem: ham and spam. Use MRJob (please include your code) to build a muiltnomial Naive Bayes classifier. Please use Laplace Smoothing with a hyperparameter of 1. Please use words only (a-z) as features. Please lowercase all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing nbtext.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile nbtext.txt\n",
    "ham d1: “good.”\n",
    "ham d2: “very good.”\n",
    "spam d3: “bad.”\n",
    "spam d4: “very bad.”\n",
    "spam d5: “very bad, very BAD.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile kldivergence_smooth.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class naive_bayes(MRJob):\n",
    "    \n",
    "    def mapper1(self, _, line):\n",
    "        \n",
    "        data = line.split(':')[-1].lower()\n",
    "        label = int(inline.split(' ')[0] = 'spam')\n",
    "        exclude = ['“','”', '.',',']\n",
    "        clean_data = ''.join(ch for ch in data if ch not in exclude)\n",
    "        clean_data_list = clean_data.split(' ')\n",
    "        wc = {}\n",
    "        for word in clean_data_list:\n",
    "            if word in wc:\n",
    "                wc[word] +=1\n",
    "            else:\n",
    "                wc[word] = 1\n",
    "        for key in count:\n",
    "            yield label, wc\n",
    "    \n",
    "\n",
    "            \n",
    "    def steps(self):\n",
    "        return [self.mr(mapper=self.mapper1\n",
    "               \n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    naive_bayes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from kldivergence_smooth import kldivergence_smooth\n",
    "mr_job = kldivergence_smooth(args=['kltext.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print(mr_job.parse_output_line(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===Weighted K-means===\n",
    "\n",
    "Write a MapReduce job in MRJob to do the training at scale of a weighted K-means algorithm.\n",
    "\n",
    "You can write your own code or you can use most of the code from the following notebook:\n",
    "\n",
    "* http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/oppgyfqxphlh69g/MrJobKmeans_Corrected.ipynb\n",
    "\n",
    "Weight each example as follows using the inverse vector length (Euclidean norm): \n",
    "\n",
    "weight(X)= 1/||X||, \n",
    "\n",
    "where ||X|| = SQRT(X.X)= SQRT(X1^2 + X2^2)\n",
    "\n",
    "Here X is vector made up of two component X1 and X2.\n",
    "\n",
    "Using the following data to answer the following TWO questions:\n",
    "\n",
    "* https://www.dropbox.com/s/ai1uc3q2ucverly/Kmeandata.csv?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Kmeans.py\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    k=3    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_init, mapper=self.mapper,combiner = self.combiner,reducer=self.reducer)\n",
    "               ]\n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        print(\"Current path:\", os.path.dirname(os.path.realpath(__file__)))\n",
    "        \n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "        #open('Centroids.txt', 'w').close()\n",
    "        \n",
    "        print(\"Centroids: \", self.centroid_points)\n",
    "        \n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        D = (map(float,line.split(',')))\n",
    "        yield int(MinDist(D,self.centroid_points)), (D[0],D[1],1)\n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        sumx = sumy = num = 0\n",
    "        for x,y,n in inputdata:\n",
    "            num = num + n\n",
    "            sumx = sumx + x\n",
    "            sumy = sumy + y\n",
    "        yield idx,(sumx,sumy,num)\n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata): \n",
    "        centroids = []\n",
    "        num = [0]*self.k \n",
    "        for i in range(self.k):\n",
    "            centroids.append([0,0])\n",
    "        for x, y, n in inputdata:\n",
    "            num[idx] = num[idx] + n\n",
    "            centroids[idx][0] = centroids[idx][0] + x\n",
    "            centroids[idx][1] = centroids[idx][1] + y\n",
    "        centroids[idx][0] = centroids[idx][0]/num[idx]\n",
    "        centroids[idx][1] = centroids[idx][1]/num[idx]\n",
    "\n",
    "        yield idx,(centroids[idx][0],centroids[idx][1])\n",
    "      \n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration0:\n",
      "Current path: C:\\Users\\timothy.weeks\\Documents\\notebooks\n",
      "Centroids:  [<map object at 0x0000000006282748>, <map object at 0x0000000006282668>, <map object at 0x00000000062826A0>]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'map' and 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-93e247ed41e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"iteration\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mmr_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_runner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[1;31m# stream_output: get access of the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\timothy.weeks\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\mrjob\\runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Job already ran!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ran_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\timothy.weeks\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\mrjob\\sim.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_counters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invoke_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mapper'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m'reducer'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\timothy.weeks\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\mrjob\\sim.py\u001b[0m in \u001b[0;36m_invoke_step\u001b[0;34m(self, step_num, step_type)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             self._run_step(step_num, step_type, input_path, output_path,\n\u001b[0;32m--> 273\u001b[0;31m                            working_dir, env)\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prev_outfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\timothy.weeks\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\mrjob\\inline.py\u001b[0m in \u001b[0;36m_run_step\u001b[0;34m(self, step_num, step_type, input_path, output_path, working_dir, env, child_stdin)\u001b[0m\n\u001b[1;32m    152\u001b[0m                     child_instance.sandbox(stdin=child_stdin,\n\u001b[1;32m    153\u001b[0m                                            stdout=child_stdout)\n\u001b[0;32m--> 154\u001b[0;31m                     \u001b[0mchild_instance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhas_combiner\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\timothy.weeks\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\mrjob\\job.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_mapper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_mapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_combiner\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\timothy.weeks\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\mrjob\\job.py\u001b[0m in \u001b[0;36mrun_mapper\u001b[0;34m(self, step_num)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[1;31m# run the mapper on each line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[1;32mfor\u001b[0m \u001b[0mout_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m                 \u001b[0mwrite_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\timothy.weeks\\Documents\\notebooks\\Kmeans.py\u001b[0m in \u001b[0;36mmapper\u001b[0;34m(self, _, line)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[1;32myield\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMinDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcentroid_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[1;31m#Combine sum of data points locally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcombiner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\timothy.weeks\\Documents\\notebooks\\Kmeans.py\u001b[0m in \u001b[0;36mMinDist\u001b[0;34m(datapoint, centroid_points)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdatapoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatapoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcentroid_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcentroid_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatapoint\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcentroid_points\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdiffsq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[1;31m# Get the nearest centroid for each instance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'map' and 'map'"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from numpy import random\n",
    "from Kmeans import MRKmeans, stop_criterion\n",
    "mr_job = MRKmeans(args=['Kmeandata.csv', '--file=Centroids.txt'])\n",
    "\n",
    "#Geneate initial centroids\n",
    "centroid_points = []\n",
    "k = 3\n",
    "for i in range(k):\n",
    "    centroid_points.append([random.uniform(-3,3),random.uniform(-3,3)])\n",
    "with open('Centroids.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "# Update centroids iteratively\n",
    "i = 0\n",
    "while(1):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print(\"iteration\"+str(i)+\":\")\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print(key, value)\n",
    "            centroid_points[key] = value\n",
    "            \n",
    "        # Update the centroids for the next iteration\n",
    "        with open('Centroids.txt', 'w') as f:\n",
    "            f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    i = i + 1\n",
    "    if(stop_criterion(centroid_points_old,centroid_points,0.01)):\n",
    "        break\n",
    "print(\"Centroids\\n\")\n",
    "print(centroid_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT18. Which result below is the closest to the centroids you got after running your weighted K-means code for K=3 for 10 iterations? \n",
    "(old11-12)\n",
    "\n",
    "* (a) (-4.0,0.0), (4.0,0.0), (6.0,6.0)     \n",
    "* (b) (-4.5,0.0), (4.5,0.0), (0.0,4.5)    \n",
    "* (c) (-5.5,0.0), (0.0,0.0), (3.0,3.0)    \n",
    "* (d) (-4.5,0.0), (-4.0,0.0), (0.0,4.5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT19. Using the result of the previous question, which number below is the closest  to the average weighted distance between each example and its assigned (closest) centroid?\n",
    "\n",
    "The average weighted distance is defined as \n",
    "sum over i  (weighted_distance_i)     /  sum over i (weight_i)\n",
    "\n",
    "\n",
    "* (a) 2.5     \n",
    "* (b) 1.5     \n",
    "* (c) 0.5     \n",
    "* (d) 4.0   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
